<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Blog Of PuppetSama"><title>浅析k近邻算法 | PuppetHut</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">浅析k近邻算法</h1><a id="logo" href="/.">PuppetHut</a><p class="description">Someday I'll be just like you.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">浅析k近邻算法</h1><div class="post-meta">Apr 21, 2018<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#k近邻简述"><span class="toc-number">1.</span> <span class="toc-text">k近邻简述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#概述"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#工作机制"><span class="toc-number">1.2.</span> <span class="toc-text">工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#懒惰学习"><span class="toc-number">1.3.</span> <span class="toc-text">懒惰学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#例图"><span class="toc-number">1.4.</span> <span class="toc-text">例图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k近邻模型"><span class="toc-number">2.</span> <span class="toc-text">k近邻模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#距离度量-L-p-距离"><span class="toc-number">2.1.</span> <span class="toc-text">距离度量:$L_{p}$距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k值的选择"><span class="toc-number">2.2.</span> <span class="toc-text">k值的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策分类规则"><span class="toc-number">2.3.</span> <span class="toc-text">决策分类规则</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k近邻实现"><span class="toc-number">3.</span> <span class="toc-text">k近邻实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kd树"><span class="toc-number">3.1.</span> <span class="toc-text">kd树</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#创建kd树"><span class="toc-number">3.1.1.</span> <span class="toc-text">创建kd树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#搜索kd树"><span class="toc-number">3.1.2.</span> <span class="toc-text">搜索kd树</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#压缩近邻算法"><span class="toc-number">3.2.</span> <span class="toc-text">压缩近邻算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#算法实现"><span class="toc-number">3.2.1.</span> <span class="toc-text">算法实现</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>数据挖掘十大算法之k近邻算法。(持续更新中，<del>等我把公式打上去</del>)</p>
<a id="more"></a>
<h2 id="k近邻简述"><a href="#k近邻简述" class="headerlink" title="k近邻简述"></a>k近邻简述</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>简单地说，k近邻算法采用测量不同特征值之间的距离的方法进行预测。</p>
<h3 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h3><p>给定测试样本，基于某种距离量度找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。<del>（近朱者赤，近墨者黑）</del></p>
<h3 id="懒惰学习"><a href="#懒惰学习" class="headerlink" title="懒惰学习"></a>懒惰学习</h3><p>k近邻学习是没有显式的训练过程。它是“懒惰学习”（lazy learning）的著名代表，此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习”（eager learning）。</p>
<h3 id="例图"><a href="#例图" class="headerlink" title="例图"></a>例图</h3><p><img src="http://omjz7so35.bkt.clouddn.com/279px-KnnClassification.svg.png" alt=""></p>
<p>k近邻算法例子。测试样本（绿色圆形）应归入要么是第一类的蓝色方形或是第二类的红色三角形。如果k=3（实线圆圈）它被分配给第二类，因为有2个三角形和只有1个正方形在内侧圆圈之内。如果k=5（虚线圆圈）它被分配到第一类（3个正方形与2个三角形在外侧圆圈之内）。</p>
<h2 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a>k近邻模型</h2><p>k值的选择，距离度量及分类决策规则是k近邻法的三个基本要素。</p>
<h3 id="距离度量-L-p-距离"><a href="#距离度量-L-p-距离" class="headerlink" title="距离度量:$L_{p}$距离"></a>距离度量:$L_{p}$距离</h3><p>设特征空间向量$\chi$是n维实数向量空间$R^{n}$，$x_{i}$，$x_{j}$$\in $ $\chi$，$x_{i}=\left(x^{\left( 1\right) }_{i,}x^{\left( 2\right) }_{i},\ldots x^{\left( n\right) }_{i}\right)^{T}$,$ x_{j}=\left(x^{\left( 1\right) }_{j,}x^{\left( 2\right) }_{j},\ldots x^{\left( n\right) }_{j}\right)^{T}$。$x_{i},x_{j}$的$L_{p}$距离定义为：</p>
<script type="math/tex; mode=display">L_{p}\left( x_{i},x_{j}\right)=\left( \sum ^{n}_{l=1}\left| x^{\left( l\right) }_{i}-x^{\left( l\right) }_{j}\right| ^{p}\right) ^{\dfrac {1}{p}}</script><p>当p=2时，称为欧氏距离(Euclidean distance)，即</p>
<script type="math/tex; mode=display">L_{2}\left( x_{i},x_{j}\right)=\left( \sum ^{n}_{l=1}\left| x^{\left( l\right) }_{i}-x^{\left( l\right) }_{j}\right| ^{2}\right) ^{\dfrac {1}{2}}</script><p>当p=1时，称为曼哈顿距离，即</p>
<script type="math/tex; mode=display">L_{1}\left( x_{i},x_{j}\right)=\sum ^{n}_{l=1}\left| x^{\left( l\right) }_{i}-x^{\left( l\right) }_{j}\right|</script><p>当p=$\infty$时，它是各个坐标距离的最大值，即</p>
<script type="math/tex; mode=display">L_{\infty}\left( x_{i},x_{j}\right)=\max _{l}\left| x^{\left( l\right) }_{i}-x^{\left( l\right) }_{j}\right|</script><p>p取不同值时与原点$L_{p}$距离为1的点的图形<br><img src="http://omjz7so35.bkt.clouddn.com/Lpdistance1208874-20171025071155848-1477111731.png" alt=""></p>
<h3 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h3><p>k值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
<p>k值的增大就意味着整体模型变得简单。</p>
<p>在应用中，k值一般取一个较小的数值，通常采用<strong>交叉验证法</strong>来选取最优的k值。<del>等一个后续详解更新</del></p>
<h3 id="决策分类规则"><a href="#决策分类规则" class="headerlink" title="决策分类规则"></a>决策分类规则</h3><p>k近邻中的分类决策往往是多数表决。</p>
<p>多数表决规则有以下解释：如果分类的损失函数为0-1损失函数，分类函数为</p>
<p>那么误分类的概率是：</p>
<p>对给定的实例____，其最近邻的k个训练实例点构成集合___，如果涵盖____的区域的类别是__，那么误分类率是</p>
<p>要使误分类率最小即经验风险最小，就要使____最大，所以多数表决规则等价于经验风险最小化。</p>
<h2 id="k近邻实现"><a href="#k近邻实现" class="headerlink" title="k近邻实现"></a>k近邻实现</h2><p>kNN的一大缺点需要存储全部训练样本，以及繁重的距离计算量。</p>
<p>改进的方案有两个</p>
<ul>
<li><p>对样本集进行组织与整理，分群分层，尽可能将计算压缩到在接近测试样本邻域的小范围内，避免盲目地与训练样本集中每个样本进行距离计算。</p>
</li>
<li><p>在原有样本集中挑选出对分类计算有效的样说本，使样本总数合理地减少，以同时达到既减少计算量，又减少存储量的双重效果。</p>
</li>
</ul>
<h3 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h3><p> kd树是k-dimension tree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，kd-树就是一种平衡二叉树。</p>
<h4 id="创建kd树"><a href="#创建kd树" class="headerlink" title="创建kd树"></a>创建kd树</h4><p>有很多种方法可以选择轴垂直分区面（ axis-aligned splitting planes ），所以有很多种创建k-d树的方法。 最典型的方法如下：</p>
<ul>
<li>随着树的深度轮流选择轴当作分区面。（例如：在三维空间中根节点是 x 轴垂直分区面，其子节点皆为 y 轴垂直分区面，其孙节点皆为 z 轴垂直分区面，其曾孙节点则皆为 x 轴垂直分区面，依此类推。）</li>
<li>点由垂直分区面之轴座标的中位数区分并放入子树</li>
</ul>
<p>这个方法产生一个平衡的k-d树。每个叶节点的高度都十分接近。然而，平衡的树不一定对每个应用都是最佳的。</p>
<p><strong>例</strong>：</p>
<p>假设有6个二维数据点{（2,3），（5,4），（9,6），（4,7），（8,1），（7,2）}，数据点位于二维空间内，构造一个平衡kd树。</p>
<p><img src="http://omjz7so35.bkt.clouddn.com/kdtreecreate1356352228_9761.jpg" alt=""></p>
<h4 id="搜索kd树"><a href="#搜索kd树" class="headerlink" title="搜索kd树"></a>搜索kd树</h4><p>k-d树最邻近搜索的过程如下：</p>
<ul>
<li>从根节点开始，递归的往下移。往左还是往右的决定方法与插入元素的方法一样(如果输入点在分区面的左边则进入左子节点，在右边则进入右子节点)。</li>
<li>一旦移动到叶节点，将该节点当作”目前最佳点”。</li>
<li>解开递归，并对每个经过的节点运行下列步骤：<ul>
<li>如果目前所在点比目前最佳点更靠近输入点，则将其变为目前最佳点。</li>
<li>检查另一边子树有没有更近的点，如果有则从该节点往下找</li>
</ul>
</li>
<li>当根节点搜索完毕后完成最邻近搜索</li>
</ul>
<p><strong>例</strong>：</p>
<p>查询（2，4.5）</p>
<p>通过二叉搜索，顺着搜索路径很快就能找到最邻近的叶子节点（4,7），首先假设（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。</p>
<p>回溯到（5,4），计算其与查找点之间的距离为3.041，小于3.202，所以“当前最近邻点”变成（5,4）。以目标点（2,4.5）为圆心，以目标点（2,4.5）到“当前最近邻点”（5,4）的距离（即3.041）为半径作圆，如下图左所示。可见该圆和y = 4超平面相交，所以需要进入（5,4）左子空间进行查找，即回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以“当前最近邻点”更新为（2,3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图右所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。</p>
<p><img src="http://omjz7so35.bkt.clouddn.com/kdtreesearch1356352671_1629.jpg" alt=""></p>
<h3 id="压缩近邻算法"><a href="#压缩近邻算法" class="headerlink" title="压缩近邻算法"></a>压缩近邻算法</h3><p>利用现有样本集，逐渐生成一个新的样本集，使该样本集在保留最少量样本的条件下，仍能对原有样本的全部用最近邻法正确分类，那么该样本集也就能对待识别样本进行分类，并保持正常识别率。</p>
<h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><p>首先定义两个存储器，一个用来存放即将生成的样本集，称为Store；另一存储器则存放原样本集，称为Grabbag。其算法是：</p>
<ol>
<li><p><strong>初始化</strong>。Store是空集，原样本集存入Grabbag；从Grabbag中任意选择一样本放入Store中作为新样本集的第一个样本。</p>
</li>
<li><p><strong>样本集生成</strong>。在Grabbag中取出第i个样本用Store中的当前样本集按最近邻法分类。若分类错误，则将该样本从Grabbag转入Store中，若分类正确，则将该样本放回Grabbag中。</p>
</li>
<li><p><strong>结束过程</strong>。若Grabbag中所有样本在执行第二步时没有发生转入Store的现象，或Grabbag已成空集，则算法终止，否则转入第二步。 </p>
</li>
</ol>
<p>参考：</p>
<blockquote>
<p><a href="https://my.oschina.net/u/1412321/blog/194174" target="_blank" rel="external">https://my.oschina.net/u/1412321/blog/194174</a><br>  <a href="https://blog.csdn.net/misaiya1/article/details/78704113" target="_blank" rel="external">https://blog.csdn.net/misaiya1/article/details/78704113</a></p>
<p>《统计学习方法》 李航</p>
<p>《机器学习》 周志华</p>
</blockquote>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://puppetkant.cn/2018/04/21/浅析k近邻算法/" data-id="cjg9jfex40012ssudp9vdas79" class="article-share-link">分享</a><div class="tags"><a href="/tags/Algorithm/">Algorithm</a></div><div class="post-nav"><a href="/2018/01/01/假装很开心/" class="next">假装很开心</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://puppetkant.cn"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Code/">Code</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Read/">Read</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/mc/" style="font-size: 15px;">mc</a> <a href="/tags/Android/" style="font-size: 15px;">Android</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Others/" style="font-size: 15px;">Others</a> <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/21/浅析k近邻算法/">浅析k近邻算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/01/假装很开心/">假装很开心</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/05/《think-python》读书笔记/">《think_python》读书笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/26/目标爬虫清单/">目标爬虫清单</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/20/Python动态爬取的两种方式/">Python动态爬取的两种方式</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/数字杭电模拟登录/">数字杭电模拟登录</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/07/Python爬坑问题集合/">Python爬坑问题集合</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/01/国内携程一日航线Python实现/">国内携程一日航线Python实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/28/思考的独立性/">思考的独立性</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/09/Python-爬虫笔记（三）——对于Lambda的认识/">Python 爬虫笔记（三）——对于Lambda的认识</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://mikumiku.com.cn/" title="Xana's blog" target="_blank">Xana's blog</a><ul></ul><a href="http://fogdong.github.io/" title="FogDong's blog" target="_blank">FogDong's blog</a><ul></ul><a href="http://yorhp.com/" title="Tyhj's blog" target="_blank">Tyhj's blog</a><ul></ul><a href="https://tecotaku.cn/" title="Sino's blog" target="_blank">Sino's blog</a><ul></ul><a href="http://hellovass.info/" title="HelloVass's blog" target="_blank">HelloVass's blog</a><ul></ul><a href="http://qingmicity.cn/" title="QingMi's blog" target="_blank">QingMi's blog</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">PuppetHut.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>